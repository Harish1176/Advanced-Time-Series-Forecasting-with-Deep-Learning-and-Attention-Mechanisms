# Advanced Time Series Forecasting with Deep Learning
# Transformer-based Multivariate Forecasting + SARIMA Baseline

# 1. Imports & Reproducibility

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.pyplot as plt
import random
import warnings
warnings.filterwarnings("ignore")

SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
random.seed(SEED)

# 2. Data Generation (Synthetic Multivariate Time Series)
# -----------------------------
def generate_multivariate_time_series(n_samples=1500):
    """
    Generates a synthetic multivariate time series with:
    - Trend
    - Seasonality
    - Noise
    - Correlated features
    """
    t = np.arange(n_samples)

    f1 = 10 + 0.02 * t + 2 * np.sin(2 * np.pi * t / 50) + np.random.normal(0, 0.5, n_samples)
    f2 = 5 + 0.7 * f1 + 1.5 * np.sin(2 * np.pi * t / 30) + np.random.normal(0, 0.4, n_samples)
    f3 = 3 + 0.5 * f1 - 0.2 * f2 + np.random.normal(0, 0.3, n_samples)

    data = np.vstack([f1, f2, f3]).T
    return pd.DataFrame(data, columns=["Feature_1", "Feature_2", "Feature_3"])

df = generate_multivariate_time_series()
print("Dataset shape:", df.shape)
df.head()

# 3. Preprocessing: Scaling & Windowing

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df.values)

class TimeSeriesDataset(Dataset):
    def _init_(self, data, input_window, forecast_horizon):
        self.data = data
        self.input_window = input_window
        self.forecast_horizon = forecast_horizon

    def _len_(self):
        return len(self.data) - self.input_window - self.forecast_horizon

    def _getitem_(self, idx):
        x = self.data[idx : idx + self.input_window]
        y = self.data[idx + self.input_window : idx + self.input_window + self.forecast_horizon]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

# Train-test split (time-aware)
train_size = int(0.8 * len(scaled_data))
train_data = scaled_data[:train_size]
test_data = scaled_data[train_size:]

INPUT_WINDOW = 30
FORECAST_HORIZON = 1
BATCH_SIZE = 32

train_dataset = TimeSeriesDataset(train_data, INPUT_WINDOW, FORECAST_HORIZON)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)


# 4. Transformer Model Definition

class TransformerTimeSeries(nn.Module):
    def _init_(self, n_features, d_model=64, n_heads=4, n_layers=2, dropout=0.1):
        super()._init_()
        self.input_projection = nn.Linear(n_features, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dropout=dropout,
            batch_first=True
        )

        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=n_layers
        )

        self.output_layer = nn.Linear(d_model, n_features)

    def forward(self, x):
        x = self.input_projection(x)
        x = self.transformer_encoder(x)
        x = x[:, -1, :]
        return self.output_layer(x)

model = TransformerTimeSeries(n_features=3)


# 5. Training Setup & Hyperparameters
# -----------------------------
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

EPOCHS = 30
train_losses = []


# 6. Model Training Loop
# -----------------------------
for epoch in range(EPOCHS):
    model.train()
    epoch_loss = 0

    for x_batch, y_batch in train_loader:
        optimizer.zero_grad()
        predictions = model(x_batch)
        loss = criterion(predictions, y_batch[:, 0, :])
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    scheduler.step()
    avg_loss = epoch_loss / len(train_loader)
    train_losses.append(avg_loss)

    print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f}")


# 7. Transformer Evaluation
# -----------------------------
model.eval()
X_test, y_test = [], []

for i in range(len(test_data) - INPUT_WINDOW - 1):
    X_test.append(test_data[i : i + INPUT_WINDOW])
    y_test.append(test_data[i + INPUT_WINDOW])

X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = np.array(y_test)

with torch.no_grad():
    transformer_preds = model(X_test).numpy()

# Metrics
def mape(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

rmse_t = np.sqrt(mean_squared_error(y_test, transformer_preds))
mae_t = mean_absolute_error(y_test, transformer_preds)
mape_t = mape(y_test, transformer_preds)


# 8. SARIMA Baseline (Univariate)
# -----------------------------
sarima_model = SARIMAX(
    train_data[:, 0],
    order=(2, 1, 2),
    seasonal_order=(1, 1, 1, 50)
)
sarima_fit = sarima_model.fit(disp=False)
sarima_forecast = sarima_fit.forecast(len(transformer_preds))

rmse_s = np.sqrt(mean_squared_error(test_data[:len(sarima_forecast), 0], sarima_forecast))
mae_s = mean_absolute_error(test_data[:len(sarima_forecast), 0], sarima_forecast)
mape_s = mape(test_data[:len(sarima_forecast), 0], sarima_forecast)


# 9. Results Summary
# -----------------------------
results = pd.DataFrame({
    "Model": ["Transformer", "SARIMA"],
    "RMSE": [rmse_t, rmse_s],
    "MAE": [mae_t, mae_s],
    "MAPE (%)": [mape_t, mape_s]
})

print("\nPerformance Comparison:")
print(results)


# 10. Visualization
# -----------------------------
plt.figure(figsize=(12, 5))
plt.plot(train_losses, label="Training Loss")
plt.title("Training Convergence (Transformer)")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.legend()
plt.show()

plt.figure(figsize=(12, 5))
plt.plot(y_test[:200, 0], label="Actual")
plt.plot(transformer_preds[:200, 0], label="Transformer Forecast")
plt.title("Forecast vs Actual (Feature 1)")
plt.legend()
plt.show()
